{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200440c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.baseline与coco-model的图片比较（均与原始text比较）\n",
    "#### 1.1 组间相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f73984f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "img_text_dic = {}\n",
    "with open(\"/home/cl/CV_Proj/evaluate/metadata_test.jsonl\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        tmp_dic = json.loads(line)\n",
    "        img_text_dic[tmp_dic[\"file_name\"]] = tmp_dic[\"text\"]\n",
    "#         print(tmp_dic[\"file_name\"],tmp_dic[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98bb6e96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6dfc72",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load('ViT-B/32')\n",
    "\n",
    "model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a25b61e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ori_img_path = '/home/cl/CV_Proj/evaluate/text2img2/text2img/original_imgs_5003'\n",
    "base_img_path = '/home/cl/CV_Proj/evaluate/text2img2/text2img/baseline_inference'\n",
    "coco_img_path = '/home/cl/CV_Proj/evaluate/text2img2/text2img/coco-model_inference'\n",
    "\n",
    "bt_img_path = '/home/cl/CV_Proj/evaluate/text2img2/text2img/coco-model-bt_inference'\n",
    "crop_img_path = '/home/cl/CV_Proj/evaluate/text2img_/coco-model-crop_swap_inference'\n",
    "b1n10_path = '/home/cl/CV_Proj/evaluate/text2img_/coco-model-linear_p_beta1_n10_inference'\n",
    "\n",
    "b1n5_img_path = '/home/cl/CV_Proj/evaluate/text2img2/text2img/coco-model-linear_p_beta1_n5_inference'\n",
    "b4n5_img_path = '/home/cl/CV_Proj/evaluate/text2img_/coco-model-linear_p_beta4_n5_inference'\n",
    "b8n5_img_path = '/home/cl/CV_Proj/evaluate/text2img_/coco-model-linear_p_beta8_n5_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c89210c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding origin images: 100%|██████████████████████████████████████████████████████████████████████████| 5003/5003 [00:42<00:00, 116.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between original images and text is: 0.15154661238193512\n"
     ]
    }
   ],
   "source": [
    "# encode图片\n",
    "img_path = '/home/cl/CV_Proj/evaluate/text2img2/text2img/original_imgs_5003'\n",
    "images = []\n",
    "text = []\n",
    "\n",
    "for i, item in enumerate(tqdm(img_text_dic,desc='Encoding origin images')):\n",
    "    cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "    cur_image = preprocess(cur_image)\n",
    "    images.append(cur_image)\n",
    "    \n",
    "    cur_text = [img_text_dic[item]]\n",
    "    text.append(cur_text)\n",
    "\n",
    "img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "#encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list).cuda()\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input)\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "\n",
    "# 计算相似度\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "print(\"The similarity between original images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fe38d31",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images: 100%|█████████████████████████████████████████████████████████████████████████████████| 5003/5003 [00:30<00:00, 162.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between baseline model generated images and text is: 0.1415233016014099\n"
     ]
    }
   ],
   "source": [
    "#encode图片\n",
    "img_path = base_img_path\n",
    "images = []\n",
    "text = []\n",
    "\n",
    "for i, item in enumerate(tqdm(img_text_dic,desc='Encoding images')):\n",
    "    cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "    cur_image = preprocess(cur_image)\n",
    "    images.append(cur_image)\n",
    "    \n",
    "    cur_text = [img_text_dic[item]]\n",
    "    text.append(cur_text)\n",
    "\n",
    "img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "# #encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list).cuda()\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input)\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "\n",
    "# #计算相似度\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "print(\"The similarity between baseline model generated images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d540928",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images: 100%|█████████████████████████████████████████████████████████████████████████████████| 5003/5003 [00:36<00:00, 137.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between coco model generated images and text is: 0.1465878039598465\n"
     ]
    }
   ],
   "source": [
    "#encode图片\n",
    "img_path = coco_img_path\n",
    "images = []\n",
    "text = []\n",
    "\n",
    "for i, item in enumerate(tqdm(img_text_dic,desc='Encoding images')):\n",
    "    cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "    cur_image = preprocess(cur_image)\n",
    "    images.append(cur_image)\n",
    "    \n",
    "    cur_text = [img_text_dic[item]]\n",
    "    text.append(cur_text)\n",
    "\n",
    "img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "# #encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list)\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input.cuda())\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "    \n",
    "\n",
    "\n",
    "# #计算相似度\n",
    "\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "print(\"The similarity between coco model generated images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "239830b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between corp_swap model generated images and text is: 0.14705707132816315\n"
     ]
    }
   ],
   "source": [
    "#encode图片\n",
    "# img_path = crop_img_path\n",
    "# images = []\n",
    "# text = []\n",
    "\n",
    "# for i, item in enumerate(tqdm(img_text_dic,desc='Encoding images')):\n",
    "#     cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "#     cur_image = preprocess(cur_image)\n",
    "#     images.append(cur_image)\n",
    "    \n",
    "#     cur_text = [img_text_dic[item]]\n",
    "#     text.append(cur_text)\n",
    "\n",
    "# img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "# #encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list).cuda()\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input)\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "\n",
    "# #计算相似度\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "    print(\"The similarity between corp_swap model generated images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50adb452",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images: 100%|█████████████████████████████████████████████████████████████████████████████████| 5003/5003 [00:42<00:00, 118.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between bt model generated images and text is: 0.1469103842973709\n"
     ]
    }
   ],
   "source": [
    "#encode图片\n",
    "img_path = bt_img_path\n",
    "images = []\n",
    "text = []\n",
    "\n",
    "for i, item in enumerate(tqdm(img_text_dic,desc='Encoding images')):\n",
    "    cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "    cur_image = preprocess(cur_image)\n",
    "    images.append(cur_image)\n",
    "    \n",
    "    cur_text = [img_text_dic[item]]\n",
    "    text.append(cur_text)\n",
    "\n",
    "img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "# #encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list).cuda()\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input)\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "\n",
    "#计算相似度\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "print(\"The similarity between bt model generated images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e79c5eba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images: 100%|█████████████████████████████████████████████████████████████████████████████████| 5003/5003 [00:35<00:00, 140.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between b1n10 model generated images and text is: 0.14710772037506104\n"
     ]
    }
   ],
   "source": [
    "#encode图片\n",
    "img_path = b1n10_path\n",
    "images = []\n",
    "text = []\n",
    "\n",
    "for i, item in enumerate(tqdm(img_text_dic,desc='Encoding images')):\n",
    "    cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "    cur_image = preprocess(cur_image)\n",
    "    images.append(cur_image)\n",
    "    \n",
    "    cur_text = [img_text_dic[item]]\n",
    "    text.append(cur_text)\n",
    "\n",
    "img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "#encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list).cuda()\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input)\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "\n",
    "#计算相似度\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "print(\"The similarity between b1n10 model generated images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c84478a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images: 100%|█████████████████████████████████████████████████████████████████████████████████| 5003/5003 [00:44<00:00, 112.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between b1n5 model generated images and text is: 0.1466483622789383\n"
     ]
    }
   ],
   "source": [
    "#encode图片\n",
    "img_path = b1n5_img_path\n",
    "images = []\n",
    "text = []\n",
    "\n",
    "for i, item in enumerate(tqdm(img_text_dic,desc='Encoding images')):\n",
    "    cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "    cur_image = preprocess(cur_image)\n",
    "    images.append(cur_image)\n",
    "    \n",
    "    cur_text = [img_text_dic[item]]\n",
    "    text.append(cur_text)\n",
    "\n",
    "img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "# #encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list).cuda()\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input)\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "\n",
    "#计算相似度\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "print(\"The similarity between b1n5 model generated images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f2e6d00",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images: 100%|█████████████████████████████████████████████████████████████████████████████████| 5003/5003 [00:30<00:00, 162.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between b4n5 model generated images and text is: 0.1467028707265854\n"
     ]
    }
   ],
   "source": [
    "#encode图片\n",
    "img_path = b4n5_img_path\n",
    "images = []\n",
    "text = []\n",
    "\n",
    "for i, item in enumerate(tqdm(img_text_dic,desc='Encoding images')):\n",
    "    cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "    cur_image = preprocess(cur_image)\n",
    "    images.append(cur_image)\n",
    "    \n",
    "    cur_text = [img_text_dic[item]]\n",
    "    text.append(cur_text)\n",
    "\n",
    "img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "#encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list).cuda()\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input)\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "\n",
    "#计算相似度\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "print(\"The similarity between b4n5 model generated images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0390c530",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images: 100%|█████████████████████████████████████████████████████████████████████████████████| 5003/5003 [00:36<00:00, 137.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between b8n5 model generated images and text is: 0.14661511778831482\n"
     ]
    }
   ],
   "source": [
    "#encode图片\n",
    "img_path = b8n5_img_path\n",
    "images = []\n",
    "text = []\n",
    "\n",
    "for i, item in enumerate(tqdm(img_text_dic,desc='Encoding images')):\n",
    "    cur_image = Image.open(os.path.join(img_path, item)).convert('RGB')\n",
    "    cur_image = preprocess(cur_image)\n",
    "    images.append(cur_image)\n",
    "    \n",
    "    cur_text = [img_text_dic[item]]\n",
    "    text.append(cur_text)\n",
    "\n",
    "img_inputs = torch.stack(images, dim=0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(img_inputs)\n",
    "    \n",
    "#encode文本\n",
    "# text_features = []\n",
    "# for idx, sent_list in enumerate(tqdm(text, desc='Encoding sentences')):\n",
    "#     try:\n",
    "#         text_input = clip.tokenize(sent_list).cuda()\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         continue\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_feature = model.encode_text(text_input)\n",
    "\n",
    "#     text_features.append(text_feature)\n",
    "\n",
    "#计算相似度\n",
    "# text_features = torch.cat(text_features, dim=0)\n",
    "with torch.no_grad():\n",
    "    img_text_sim = F.cosine_similarity(text_features.unsqueeze(1).cpu().float(), img_features.unsqueeze(0).cpu().float(), dim=-1).mean()\n",
    "print(\"The similarity between b8n5 model generated images and text is:\",img_text_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627e5e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87ba7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1c44b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}